{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduction of  `epsilon` metric\n",
    "\n",
    "If you wish to reproduce the results presented in our paper from scratch, feel free to use the below code.\n",
    "In this notebook, we provide the codes to reproduce the results for NAS-Bench-201 sarch space, CIFAR10, CIFAR100 and ImageNet16-120 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from tqdm import trange\n",
    "from dotmap import DotMap\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import nasspace\n",
    "from datasets import data\n",
    "from epsilon_utils import prepare_seed, compute_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the dataset. Choose among: 'cifar10', 'cifar100', 'ImageNet16-120'\n",
    "dataset = 'cifar100'\n",
    "\n",
    "if dataset=='ImageNet16-120':\n",
    "    data_loc = './datasets/ImageNet16'\n",
    "else:\n",
    "    data_loc = './datasets/cifardata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "repeat=1\n",
    "GPU='1'\n",
    "augtype='none'\n",
    "trainval=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments required for NAS-Bench-201 search space initialisation\n",
    "args = DotMap()\n",
    "args.nasspace = 'nasbench201'\n",
    "args.dataset=dataset\n",
    "args.api_loc = './api/NAS-Bench-201-v1_1-096897.pth'\n",
    "\n",
    "savedataset = dataset\n",
    "dataset = 'fake' if 'fake' in savedataset else savedataset\n",
    "savedataset = savedataset.replace('fake', '')\n",
    "if savedataset == 'cifar10':\n",
    "    savedataset = savedataset + '-valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./api/NAS-Bench-201-v1_1-096897.pth\n"
     ]
    }
   ],
   "source": [
    "# Load the search space (it takes some time)\n",
    "searchspace = nasspace.get_search_space(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'valid' in savedataset:\n",
    "    savedataset = savedataset.replace('-valid', '')\n",
    "\n",
    "if dataset == 'cifar10':\n",
    "    acc_type = 'ori-test'\n",
    "    val_acc_type = 'x-valid'\n",
    "else:\n",
    "    acc_type = 'x-test'\n",
    "    val_acc_type = 'x-valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "prepare_seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# The first time, data will be downloaded into \n",
    "train_loader = data.get_data(dataset, data_loc, trainval, batch_size, augtype, repeat, args)\n",
    "\n",
    "# Pick up a batch\n",
    "data_iterator = iter(train_loader)\n",
    "x, _ = next(data_iterator) # No need of true labels\n",
    "x = x.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the computation of `epsilon` metric for all the architectures within the search space. There are 15,265 architectures in NAS-Bench-201."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 15625/15625 [30:40<00:00,  8.49it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dir = './release_results/{}/'.format(dataset.upper())\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "weights = [1e-7, 1]\n",
    "test_accs = []\n",
    "val_accs = []\n",
    "nparams = []\n",
    "scores = []\n",
    "for i in trange(len(searchspace)):\n",
    "    start = time.time()\n",
    "    uid = searchspace[i]\n",
    "    network = searchspace.get_network(uid)\n",
    "    network = network.to(device)\n",
    "    score = compute_epsilon(x, network, weights)\n",
    "    scores.append(score)\n",
    "    nparams.append(sum(p.numel() for p in network.parameters()))\n",
    "    test_accs.append(searchspace.get_final_accuracy(uid, acc_type, False))\n",
    "    if dataset=='cifar10':\n",
    "        val_accs.append(searchspace.get_final_accuracy(uid, val_acc_type, True))\n",
    "    else:\n",
    "        val_accs.append(searchspace.get_final_accuracy(uid, val_acc_type, False))\n",
    "    times.append(time.time()-start)\n",
    "\n",
    "# Save your results\n",
    "save_dic = {} \n",
    "save_dic[\"scores\"] = scores\n",
    "save_dic[\"nparams\"] = nparams\n",
    "save_dic[\"test_accs\"] = test_accs\n",
    "save_dic[\"val_accs\"] = val_accs\n",
    "save_dic[\"times\"] = times\n",
    "\n",
    "pkl.dump(save_dic, open(save_dir + \"Data\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mean acc: 90.45%, mean rank: 6.19: 100%|███████████████████████████████████████████████████████| 500/500 [1:26:40<00:00, 10.40s/it]\n"
     ]
    }
   ],
   "source": [
    "save_dir = './release_results/{}/avrg_perform/'.format(dataset.upper(), batch_size)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "weights = [1e-7, 1]\n",
    "n_runs = 500\n",
    "n_samples = 100\n",
    "ind_actual_best_mean = 0\n",
    "\n",
    "times = []\n",
    "chosen = []\n",
    "accs = []\n",
    "val_accs = []\n",
    "topscores = []\n",
    "\n",
    "# Set up the log file\n",
    "if trainval:\n",
    "    logs_filename = save_dir + 'logs_' + dataset.upper() +  'val_100samples.txt'\n",
    "else:\n",
    "    logs_filename = save_dir + 'logs_' + dataset.upper() +  '_100samples.txt'\n",
    "\n",
    "with open(logs_filename, 'w') as logs:\n",
    "    runs = trange(n_runs, desc='acc: ')\n",
    "    for N in runs:\n",
    "        start = time.time()\n",
    "        scores = []\n",
    "        accs_run = []\n",
    "        nparams = []\n",
    "        np.random.seed(N)\n",
    "        indices = np.random.randint(0, len(searchspace), n_samples)\n",
    "        for i in indices:\n",
    "            uid = searchspace[i]\n",
    "            network = searchspace.get_network(uid)\n",
    "            network = network.to(device)\n",
    "            score = compute_epsilon(x, network, weights)\n",
    "            scores.append(score)\n",
    "            nparams.append(sum(p.numel() for p in network.parameters()))\n",
    "            accs_run.append(searchspace.get_final_accuracy(uid, acc_type, trainval))\n",
    "\n",
    "        accs_run.sort(reverse=True)\n",
    "        best_arch = indices[np.nanargmax(scores)]\n",
    "        uid_best = searchspace[best_arch]\n",
    "        ind_actual_best = accs_run.index(searchspace.get_final_accuracy(uid_best, acc_type, trainval))\n",
    "        ind_actual_best_mean += ind_actual_best\n",
    "\n",
    "        topscores.append(scores[np.nanargmax(scores)])\n",
    "        chosen.append(best_arch)\n",
    "        accs.append(searchspace.get_final_accuracy(uid_best, acc_type, trainval))\n",
    "\n",
    "        if not dataset == 'cifar10' or trainval:\n",
    "            val_accs.append(searchspace.get_final_accuracy(uid_best, val_acc_type, trainval))\n",
    "        logs.write(f\"Mean acc: {mean(accs if not trainval else val_accs):.2f}% \")\n",
    "        logs.write(f\"Actual ranking: {ind_actual_best} \\n\")\n",
    "        times.append(time.time()-start)\n",
    "        runs.set_description(f\"mean acc: {mean(accs if not trainval else val_accs):.2f}%, mean rank: {ind_actual_best_mean/(N+1):.2f}\")\n",
    "\n",
    "    logs.write(f\"Average chosen architecure's rank: {ind_actual_best_mean/n_runs} \\n\")\n",
    "    logs.write(f\"Final mean test accuracy: {np.mean(accs)} +- {np.std(accs)} \\n\")\n",
    "    logs.write(f\"Median duration: {np.median(times)} \\n\")\n",
    "    if len(val_accs) > 1:\n",
    "        logs.write(f\"Final mean validation accuracy: {np.mean(val_accs)} +- {np.std(val_accs)} \\n\")\n",
    "\n",
    "state = {'accs': accs,\n",
    "         'val_accs': val_accs,\n",
    "         'chosen': chosen,\n",
    "         'times': times,\n",
    "         'topscores': topscores,\n",
    "         }\n",
    "\n",
    "# Save your results\n",
    "save_dic = {} \n",
    "save_dic[\"accs\"] = accs\n",
    "save_dic[\"val_accs\"] = val_accs\n",
    "save_dic[\"chosen\"] = chosen\n",
    "save_dic[\"times\"] = times\n",
    "save_dic[\"topscores\"] = topscores\n",
    "\n",
    "pkl.dump(save_dic, open(save_dir + \"Data_500runs_100samples_TRAINVAL\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mean acc: 71.79%, mean rank: 8.19: 100%|████████████████████████████████████████████████████████| 500/500 [00:01<00:00, 279.19it/s]\n"
     ]
    }
   ],
   "source": [
    "n_runs = 500\n",
    "n_samples = 1000\n",
    "ind_actual_best_mean = 0\n",
    "\n",
    "times_run = []\n",
    "chosen = []\n",
    "accs = []\n",
    "val_accs = []\n",
    "topscores = []\n",
    "opt_test = []\n",
    "opt_val = []\n",
    "rand_test = []\n",
    "rand_val = []\n",
    "\n",
    "# Read the data\n",
    "if dataset=='cifar10':\n",
    "    f = open('/home/gracheva/Work/NAS/Epsilon-NAS/NAS-Bench-201/release_results/CIFAR10/Data','rb')\n",
    "elif dataset=='cifar100':\n",
    "    f = open('/home/gracheva/Work/NAS/Epsilon-NAS/NAS-Bench-201/release_results/CIFAR100/Data','rb')\n",
    "elif dataset=='ImageNet16-120':\n",
    "    f = open('/home/gracheva/Work/NAS/Epsilon-NAS/NAS-Bench-201/release_results/IMAGENET16-120/Data','rb')\n",
    "while(1):\n",
    "    try:\n",
    "        d = pkl.load(f)\n",
    "        scores_all = d['scores']\n",
    "        accs_all = d['test_accs']\n",
    "        val_accs_all = d['val_accs']\n",
    "        times = d['times']\n",
    "    except EOFError:\n",
    "        break\n",
    "f.close()\n",
    "    \n",
    "# Set up the log file\n",
    "save_dir = './release_results/average_performance/{}/epsilon/'.format(dataset.upper())\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "logs_filename = '{}logs_{}_{}samples.txt'.format(save_dir, dataset.upper(), n_samples)\n",
    "\n",
    "with open(logs_filename, 'w') as logs:\n",
    "    runs = trange(n_runs, desc='acc: ')\n",
    "    for N in runs:\n",
    "        scores_run = []\n",
    "        accs_run = []\n",
    "        nparams = []\n",
    "        time_run = 0\n",
    "        np.random.seed(N)\n",
    "        indices = np.random.randint(0, len(accs_all), n_samples)\n",
    "        for i in indices:\n",
    "            scores_run.append(scores_all[i])\n",
    "            accs_run.append(accs_all[i])\n",
    "            time_run += times[i]\n",
    "            \n",
    "        # Computing optimal, random accuracies\n",
    "        opt_test.append(np.nanmax(accs_run))\n",
    "        opt_val.append(val_accs_all[indices[np.nanargmax(accs_run)]])\n",
    "        rand_test.append(accs_run[0])\n",
    "        rand_val.append(val_accs_all[indices[0]])\n",
    "        \n",
    "        accs_run.sort(reverse=True)\n",
    "        best_arch = indices[np.nanargmax(scores_run)]\n",
    "        ind_actual_best = accs_run.index(accs_all[best_arch])\n",
    "        ind_actual_best_mean += ind_actual_best\n",
    "\n",
    "        topscores.append(np.nanmax(scores_run))\n",
    "        chosen.append(best_arch)\n",
    "        accs.append(accs_all[best_arch])\n",
    "        val_accs.append(val_accs_all[best_arch])        \n",
    "        \n",
    "        logs.write(f\"Mean acc: {mean(accs):.2f}% \")\n",
    "        logs.write(f\"Actual ranking: {ind_actual_best} \\n\")\n",
    "        times_run.append(time_run)\n",
    "        runs.set_description(f\"mean acc: {mean(accs):.2f}%, mean rank: {ind_actual_best_mean/(N+1):.2f}\")\n",
    "\n",
    "    logs.write(f\"Average chosen architecure's rank: {ind_actual_best_mean/n_runs} \\n\")\n",
    "    logs.write(f\"Final mean test accuracy: {np.mean(accs)} +- {np.std(accs)} \\n\")\n",
    "    logs.write(f\"Median duration: {np.median(times_run)} \\n\")\n",
    "    logs.write(f\"Final mean validation accuracy: {np.mean(val_accs)} +- {np.std(val_accs)} \\n\\n\")\n",
    "    logs.write(f\"Final mean optimal test accuracy: {np.mean(opt_test)} +- {np.std(opt_test)} \\n\")\n",
    "    logs.write(f\"Final mean optimal validation accuracy: {np.mean(opt_val)} +- {np.std(opt_val)} \\n\")\n",
    "    \n",
    "    logs.write(f\"Final mean random test accuracy: {np.mean(rand_test)} +- {np.std(rand_test)} \\n\")\n",
    "    logs.write(f\"Final mean random validation accuracy: {np.mean(rand_val)} +- {np.std(rand_val)} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2732"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[np.nanargmax(accs_run)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to run the statistics over these results in `NAS-Bench-201 Stats.ipynb`notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
