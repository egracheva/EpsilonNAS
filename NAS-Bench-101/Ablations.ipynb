{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation studies\n",
    "\n",
    "In this notebook we give the scripts for various ablation studies resented in out paper:\n",
    "- **Weights**<br>\n",
    "    Verifying the role of the pair of constant shared weights used for `epsilon` computation.<br>\n",
    "- **Batch size**<br>\n",
    "    Investigating the effect of the batch size.<br>\n",
    "- **Synthetic data**<br>\n",
    "    Changing the input data to verify dependency of the method on the data.\n",
    "- **Initializations**<br>\n",
    "    Changing initialization scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-17 10:02:59.570159: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.1/extras/CUPTI/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.1/extras/CUPTI/lib64\n",
      "2023-01-17 10:02:59.570280: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.1/extras/CUPTI/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.1/extras/CUPTI/lib64\n",
      "2023-01-17 10:02:59.570292: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from tqdm import trange\n",
    "from dotmap import DotMap\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import nasspace\n",
    "from datasets import data\n",
    "from epsilon_utils import prepare_seed, compute_stats, plot_results, search_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight ablations\n",
    "\n",
    "In this ablation study we verify how much does the performance of epsilon metric change depending on the chosen weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'cifar10'\n",
    "data_loc = './datasets/cifardata'\n",
    "batch_size = 256\n",
    "repeat = 1\n",
    "GPU = '0'\n",
    "augtype = 'none'\n",
    "trainval = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments required for NAS-Bench-201 search space initialisation\n",
    "args = DotMap()\n",
    "\n",
    "args.api_loc = './nasbench_only108.tfrecord'\n",
    "args.nasspace = 'nasbench101'\n",
    "args.dataset = dataset\n",
    "args.stem_out_channels = 128\n",
    "args.num_stacks = 3\n",
    "args.num_modules_per_stack = 3\n",
    "args.num_labels = 1\n",
    "\n",
    "savedataset = dataset\n",
    "dataset = 'fake' if 'fake' in savedataset else savedataset\n",
    "savedataset = savedataset.replace('fake', '')\n",
    "if savedataset == 'cifar10':\n",
    "    savedataset = savedataset + '-valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from file... This may take a few minutes...\n",
      "Loaded dataset in 63 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load the search space (it takes some time)\n",
    "searchspace = nasspace.get_search_space(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'valid' in savedataset:\n",
    "    savedataset = savedataset.replace('-valid', '')\n",
    "    \n",
    "if args.dataset == 'cifar10':\n",
    "    acc_type = 'ori-test'\n",
    "    val_acc_type = 'x-valid'\n",
    "else:\n",
    "    acc_type = 'x-test'\n",
    "    val_acc_type = 'x-valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Randomly select n_samples architectures\n",
    "prepare_seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./datasets/cifardata/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1197522e281404f918af28c7c207886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed download. Trying https -> http instead. Downloading http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./datasets/cifardata/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e590d137a9c49e98adc318f2d9c6467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/cifardata/cifar-10-python.tar.gz to ./datasets/cifardata\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load the data \n",
    "train_loader = data.get_data(dataset, data_loc, trainval, batch_size, augtype, repeat, args)\n",
    "\n",
    "# Pick up a batch\n",
    "data_iterator = iter(train_loader)\n",
    "x, _= next(data_iterator)\n",
    "x = x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weight_range = [1e-4, 1e-3, 1e-2, 1e-1, 1, 10]\n",
    "it = 0\n",
    "for weight_comb in itertools.product(weight_range, weight_range):\n",
    "    weight_l = weight_comb[0]\n",
    "    weight_h = weight_comb[1]\n",
    "    \n",
    "    if weight_h>weight_l:\n",
    "        save_dir = './release_results/NAS-Bench-101/Ablation/Weights/Weight_{}_{}/BS{}/'.format(weight_l, weight_h, args.batch_size)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        if os.path.exists(save_dir+'Data'):\n",
    "            # Load precomputed results\n",
    "            data_file = open(save_dir+'Data','rb')\n",
    "            input_data = pkl.load(data_file)\n",
    "            score = input_data[\"score\"]\n",
    "            accs_mean = input_data[\"accs_mean\"]\n",
    "            accs_min = input_data[\"accs_min\"]\n",
    "            accs_max = input_data[\"accs_max\"]\n",
    "            nparams = input_data[\"nparams\"]\n",
    "        else:\n",
    "            accs_mean = []\n",
    "            accs_min = []\n",
    "            accs_max = []\n",
    "            nparams = []\n",
    "            score = []\n",
    "            for i in trange(5000):\n",
    "                uid = searchspace[i]\n",
    "                network = searchspace.get_network(uid)\n",
    "                network = network.to(device)\n",
    "                preds = []\n",
    "                for weight in [weight_l, weight_h]:\n",
    "                    \n",
    "                    torch.cuda.empty_cache()\n",
    "                    prepare_seed(21)\n",
    "                    \n",
    "                    # Initialize\n",
    "                    def initialize_resnet(m):\n",
    "                        if type(m) == torch.nn.Sequential:\n",
    "                            for sub_m in m:\n",
    "                                initialize_resnet(sub_m)\n",
    "                        else:\n",
    "                            fill_bias = False\n",
    "                            if hasattr(m, 'bias'):\n",
    "                                if m.bias is not None:\n",
    "                                    fill_bias = True\n",
    "\n",
    "                            if fill_bias:\n",
    "                                torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "                            fill_weight = False\n",
    "                            if hasattr(m, 'weight'):\n",
    "                                fill_weight = True\n",
    "\n",
    "                            if hasattr(m, 'affine'):\n",
    "                                if not m.affine:\n",
    "                                    fill_weight = False\n",
    "\n",
    "                            if fill_weight:\n",
    "                                torch.nn.init.constant_(m.weight, weight)\n",
    "\n",
    "                    network.apply(initialize_resnet)\n",
    "                    y_pred, _ = network(x)\n",
    "                    pred = y_pred.cpu().detach().numpy().flatten()\n",
    "                    pred_min = np.nanmin(pred)\n",
    "                    pred_max = np.nanmax(pred)\n",
    "                    pred_norm = (pred - pred_min)/(pred_max - pred_min)\n",
    "                    preds.append(pred_norm)\n",
    "\n",
    "                # Compute the score\n",
    "                preds = np.array(preds)\n",
    "                preds[np.where(preds==0)] = np.nan\n",
    "                mae = np.nanmean(np.abs(preds[0,:]-preds[1,:]))\n",
    "                mean = np.nanmean(preds)\n",
    "\n",
    "                score.append(mae/mean)\n",
    "                nparams.append(sum(p.numel() for p in network.parameters()))\n",
    "                accs_mean.append(searchspace.get_final_accuracy(uid, acc_type, args.trainval)[0])\n",
    "                accs_min.append(searchspace.get_final_accuracy(uid, acc_type, args.trainval)[1])\n",
    "                accs_max.append(searchspace.get_final_accuracy(uid, acc_type, args.trainval)[2]) \n",
    "\n",
    "            save_dic = {}\n",
    "            save_dic[\"score\"] = score\n",
    "            save_dic[\"accs_mean\"] = accs_mean\n",
    "            save_dic[\"accs_min\"] = accs_min\n",
    "            save_dic[\"accs_max\"] = accs_max\n",
    "            save_dic[\"nparams\"] = nparams\n",
    "            pkl.dump(save_dic, open(save_dir + \"Data\", \"wb\"))\n",
    "            \n",
    "        if it==0:\n",
    "            # Make the table\n",
    "            headers = [\"Weights\", \"Archs\", \"Spearman (global)\", \"Spearman (top-10%)\", \"Kendall (global)\", \"Kendall (top-10%)\", \"Top-10%/top-10%\", \"Top-64/top-5%\"]\n",
    "            table = PrettyTable(headers)\n",
    "        stats, remain = compute_stats(score, accs_mean)\n",
    "        stats_print = [\"[{}, {}]\".format(weight_l, weight_h), remain] + stats     \n",
    "        table.add_row(stats_print)\n",
    "        it+=1\n",
    "        \n",
    "#         plot_results(score=score,\n",
    "#                      accs=accs_mean,\n",
    "#                      nparams=nparams,\n",
    "#                      top10=False,\n",
    "#                      log_scale=False,\n",
    "#                      save_dir=save_dir,\n",
    "#                      save_name=\"Epsilon_{}_{}.png\".format(weight_l, weight_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = search_weights(x, searchspace, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0001, 0.001]</td>\n",
       "      <td>2120</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.28</td>\n",
       "      <td>38.21</td>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0001, 0.01]</td>\n",
       "      <td>2120</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.02</td>\n",
       "      <td>41.98</td>\n",
       "      <td>24.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0001, 0.1]</td>\n",
       "      <td>2120</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.02</td>\n",
       "      <td>41.98</td>\n",
       "      <td>24.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0001, 1]</td>\n",
       "      <td>2120</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.02</td>\n",
       "      <td>41.98</td>\n",
       "      <td>24.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0001, 10]</td>\n",
       "      <td>2120</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.02</td>\n",
       "      <td>41.98</td>\n",
       "      <td>24.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.001, 0.01]</td>\n",
       "      <td>4968</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>17.30</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.001, 0.1]</td>\n",
       "      <td>4968</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>17.30</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.001, 1]</td>\n",
       "      <td>4968</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>17.30</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.001, 10]</td>\n",
       "      <td>4968</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>17.30</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.01, 0.1]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.01, 1]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.01, 10]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.1, 1]</td>\n",
       "      <td>5000</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.1, 10]</td>\n",
       "      <td>5000</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[1, 10]</td>\n",
       "      <td>5000</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0     1      2      3      4      5      6      7\n",
       "0   [0.0001, 0.001]  2120   0.47   0.41   0.33   0.28  38.21  23.00\n",
       "1    [0.0001, 0.01]  2120   0.61   0.03   0.43   0.02  41.98  24.00\n",
       "2     [0.0001, 0.1]  2120   0.61   0.03   0.43   0.02  41.98  24.00\n",
       "3       [0.0001, 1]  2120   0.61   0.03   0.43   0.02  41.98  24.00\n",
       "4      [0.0001, 10]  2120   0.61   0.03   0.43   0.02  41.98  24.00\n",
       "5     [0.001, 0.01]  4968   0.29  -0.05   0.20  -0.03  17.30  10.00\n",
       "6      [0.001, 0.1]  4968   0.29  -0.05   0.19  -0.03  17.30   8.00\n",
       "7        [0.001, 1]  4968   0.29  -0.05   0.19  -0.03  17.30   8.00\n",
       "8       [0.001, 10]  4968   0.29  -0.05   0.19  -0.03  17.30   8.00\n",
       "9       [0.01, 0.1]  5000   0.21  -0.12   0.14  -0.08   4.60   0.00\n",
       "10        [0.01, 1]  5000   0.21  -0.12   0.14  -0.08   4.60   0.00\n",
       "11       [0.01, 10]  5000   0.21  -0.12   0.14  -0.08   4.60   0.00\n",
       "12         [0.1, 1]  5000  -0.47   0.04  -0.32   0.03   0.20   0.00\n",
       "13        [0.1, 10]  5000  -0.45   0.00  -0.30   0.00   0.20   0.00\n",
       "14          [1, 10]  5000  -0.44   0.01  -0.29   0.00   0.20   0.00"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(table.rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0001, 0.001]</td>\n",
       "      <td>2120</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.28</td>\n",
       "      <td>38.21</td>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0001, 0.01]</td>\n",
       "      <td>2120</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.02</td>\n",
       "      <td>41.98</td>\n",
       "      <td>24.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0001, 0.1]</td>\n",
       "      <td>2120</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.02</td>\n",
       "      <td>41.98</td>\n",
       "      <td>24.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0001, 1]</td>\n",
       "      <td>2120</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.02</td>\n",
       "      <td>41.98</td>\n",
       "      <td>24.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0001, 10]</td>\n",
       "      <td>2120</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.02</td>\n",
       "      <td>41.98</td>\n",
       "      <td>24.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.001, 0.01]</td>\n",
       "      <td>4968</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>17.30</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.001, 0.1]</td>\n",
       "      <td>4968</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>17.30</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.001, 1]</td>\n",
       "      <td>4968</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>17.30</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.001, 10]</td>\n",
       "      <td>4968</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>17.30</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.01, 0.1]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.01, 1]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.01, 10]</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.1, 1]</td>\n",
       "      <td>5000</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.1, 10]</td>\n",
       "      <td>5000</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[1, 10]</td>\n",
       "      <td>5000</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0     1      2      3      4      5      6      7\n",
       "0   [0.0001, 0.001]  2120   0.47   0.41   0.33   0.28  38.21  23.00\n",
       "1    [0.0001, 0.01]  2120   0.61   0.03   0.43   0.02  41.98  24.00\n",
       "2     [0.0001, 0.1]  2120   0.61   0.03   0.43   0.02  41.98  24.00\n",
       "3       [0.0001, 1]  2120   0.61   0.03   0.43   0.02  41.98  24.00\n",
       "4      [0.0001, 10]  2120   0.61   0.03   0.43   0.02  41.98  24.00\n",
       "5     [0.001, 0.01]  4968   0.29  -0.05   0.20  -0.03  17.30  10.00\n",
       "6      [0.001, 0.1]  4968   0.29  -0.05   0.19  -0.03  17.30   8.00\n",
       "7        [0.001, 1]  4968   0.29  -0.05   0.19  -0.03  17.30   8.00\n",
       "8       [0.001, 10]  4968   0.29  -0.05   0.19  -0.03  17.30   8.00\n",
       "9       [0.01, 0.1]  5000   0.21  -0.12   0.14  -0.08   4.60   0.00\n",
       "10        [0.01, 1]  5000   0.21  -0.12   0.14  -0.08   4.60   0.00\n",
       "11       [0.01, 10]  5000   0.21  -0.12   0.14  -0.08   4.60   0.00\n",
       "12         [0.1, 1]  5000  -0.47   0.04  -0.32   0.03   0.20   0.00\n",
       "13        [0.1, 10]  5000  -0.45   0.00  -0.30   0.00   0.20   0.00\n",
       "14          [1, 10]  5000  -0.44   0.01  -0.29   0.00   0.20   0.00"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(table.rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 1/5000 [00:00<24:54,  3.35it/s]/home/gracheva/miniconda3/envs/tf2/lib/python3.7/site-packages/ipykernel_launcher.py:87: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/gracheva/miniconda3/envs/tf2/lib/python3.7/site-packages/ipykernel_launcher.py:93: RuntimeWarning: Mean of empty slice\n",
      "100%|███████████████████████████████████████| 5000/5000 [24:23<00:00,  3.42it/s]\n",
      "100%|███████████████████████████████████████| 5000/5000 [24:33<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|████████████▍                          | 1593/5000 [19:44<30:49,  1.84it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████| 5000/5000 [1:03:04<00:00,  1.32it/s]\n",
      "  1%|▏                                      | 26/5000 [00:22<1:43:20,  1.25s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 70%|███████████████████████████▎           | 3499/5000 [44:18<12:39,  1.98it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|█████████████████████████████████████| 5000/5000 [1:03:06<00:00,  1.32it/s]\n",
      " 26%|██████████▏                            | 1305/5000 [16:08<32:15,  1.91it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 96%|███████████████████████████████████▋ | 4822/5000 [1:00:56<02:07,  1.40it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 68%|██████████████████████████▍            | 3384/5000 [42:50<27:17,  1.01s/it]"
     ]
    }
   ],
   "source": [
    "batch_size_range = [256, 1024]\n",
    "\n",
    "weight_l = 0.001\n",
    "weight_h = 10\n",
    "\n",
    "batch_all_spearman_all = []\n",
    "batch_top10_spearman_all = []\n",
    "batch_all_kendall_all = []\n",
    "batch_top10_kendall_all = []\n",
    "batch_top10top10_all = []\n",
    "batch_top64top5_all = []\n",
    "\n",
    "for batch_size in batch_size_range:\n",
    "    batch_all_spearman = []\n",
    "    batch_top10_spearman = []\n",
    "    batch_all_kendall = []\n",
    "    batch_top10_kendall = []\n",
    "    batch_top10top10 = []\n",
    "    batch_top64top5 = []\n",
    "    \n",
    "    for it in range(10):\n",
    "        # Load the data batch\n",
    "        if it==0:\n",
    "            train_loader = data.get_data(args.dataset, args.data_loc, args.trainval, batch_size, args.augtype, args.repeat, args)\n",
    "        data_iterator = iter(train_loader)\n",
    "        x, _= next(data_iterator)\n",
    "        x = x.to(device)\n",
    "\n",
    "        save_dir = './release_results/NAS-Bench-101/Ablation/Batch Size/BS{}/'.format(batch_size)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        if os.path.exists(save_dir+'Data_' + str(it)):\n",
    "            # Load precomputed results\n",
    "            data_file = open(save_dir + 'Data_' + str(it),'rb')\n",
    "            input_data = pkl.load(data_file)\n",
    "            score = input_data[\"score\"]\n",
    "            accs_mean = input_data[\"accs_mean\"]\n",
    "            accs_min = input_data[\"accs_min\"]\n",
    "            accs_max = input_data[\"accs_max\"]\n",
    "            nparams = input_data[\"nparams\"]\n",
    "        else:\n",
    "            accs_mean = []\n",
    "            accs_min = []\n",
    "            accs_max = []\n",
    "            nparams = []\n",
    "            score = []\n",
    "            for i in trange(5000):\n",
    "                uid = searchspace[i]\n",
    "                network = searchspace.get_network(uid)\n",
    "                network = network.to(device)\n",
    "                preds = []\n",
    "                for weight in [weight_l, weight_h]:\n",
    "\n",
    "                    torch.cuda.empty_cache()\n",
    "                    prepare_seed(21)\n",
    "\n",
    "                    # Initialize\n",
    "                    def initialize_resnet(m):\n",
    "                        if type(m) == torch.nn.Sequential:\n",
    "                            for sub_m in m:\n",
    "                                initialize_resnet(sub_m)\n",
    "                        else:\n",
    "                            fill_bias = False\n",
    "                            if hasattr(m, 'bias'):\n",
    "                                if m.bias is not None:\n",
    "                                    fill_bias = True\n",
    "\n",
    "                            if fill_bias:\n",
    "                                torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "                            fill_weight = False\n",
    "                            if hasattr(m, 'weight'):\n",
    "                                fill_weight = True\n",
    "\n",
    "                            if hasattr(m, 'affine'):\n",
    "                                if not m.affine:\n",
    "                                    fill_weight = False\n",
    "\n",
    "                            if fill_weight:\n",
    "                                torch.nn.init.constant_(m.weight, weight)\n",
    "\n",
    "                    network.apply(initialize_resnet)\n",
    "                    y_pred, _ = network(x)\n",
    "                    pred = y_pred.cpu().detach().numpy().flatten()\n",
    "                    pred_min = np.nanmin(pred)\n",
    "                    pred_max = np.nanmax(pred)\n",
    "                    pred_norm = (pred - pred_min)/(pred_max - pred_min)\n",
    "                    preds.append(pred_norm)\n",
    "\n",
    "                # Compute the score\n",
    "                preds = np.array(preds)\n",
    "                preds[np.where(preds==0)] = np.nan\n",
    "                mae = np.nanmean(np.abs(preds[0,:]-preds[1,:]))\n",
    "                mean = np.nanmean(preds)\n",
    "\n",
    "                score.append(mae/mean)\n",
    "                nparams.append(sum(p.numel() for p in network.parameters()))\n",
    "                accs_mean.append(searchspace.get_final_accuracy(uid, acc_type, args.trainval)[0])\n",
    "                accs_min.append(searchspace.get_final_accuracy(uid, acc_type, args.trainval)[1])\n",
    "                accs_max.append(searchspace.get_final_accuracy(uid, acc_type, args.trainval)[2])\n",
    "\n",
    "            save_dic = {}\n",
    "            save_dic[\"score\"] = score\n",
    "            save_dic[\"accs_mean\"] = accs_mean\n",
    "            save_dic[\"accs_min\"] = accs_min\n",
    "            save_dic[\"accs_max\"] = accs_max\n",
    "            save_dic[\"nparams\"] = nparams\n",
    "            pkl.dump(save_dic, open(save_dir + \"Data_\" + str(it), \"wb\"))\n",
    "\n",
    "        stats, remain = compute_stats(score, accs_mean, raw=True)\n",
    "        batch_all_spearman.append(stats[0])\n",
    "        batch_top10_spearman.append(stats[1])\n",
    "        batch_all_kendall.append(stats[2])\n",
    "        batch_top10_kendall.append(stats[3])\n",
    "        batch_top10top10.append(stats[4])\n",
    "        batch_top64top5.append(stats[5])\n",
    "\n",
    "    batch_all_spearman_all.append(batch_all_spearman)\n",
    "    batch_top10_spearman_all.append(batch_top10_spearman)\n",
    "    batch_all_kendall_all.append(batch_all_kendall)\n",
    "    batch_top10_kendall_all.append(batch_top10_kendall)\n",
    "    batch_top10top10_all.append(batch_top10top10)\n",
    "    batch_top64top5_all.append(batch_top64top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_range = [8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "\n",
    "weight_l = 0.0001\n",
    "weight_h = 10\n",
    "\n",
    "batch_all_spearman_all = []\n",
    "batch_top10_spearman_all = []\n",
    "batch_all_kendall_all = []\n",
    "batch_top10_kendall_all = []\n",
    "batch_top10top10_all = []\n",
    "batch_top64top5_all = []\n",
    "\n",
    "for batch_size in batch_size_range:\n",
    "    batch_all_spearman = []\n",
    "    batch_top10_spearman = []\n",
    "    batch_all_kendall = []\n",
    "    batch_top10_kendall = []\n",
    "    batch_top10top10 = []\n",
    "    batch_top64top5 = []\n",
    "    \n",
    "    for it in range(10):\n",
    "        # Load the data batch\n",
    "        if it==0:\n",
    "            train_loader = data.get_data(args.dataset, args.data_loc, args.trainval, batch_size, args.augtype, args.repeat, args)\n",
    "        data_iterator = iter(train_loader)\n",
    "        x, _= next(data_iterator)\n",
    "        x = x.to(device)\n",
    "\n",
    "        save_dir = './release_results/NAS-Bench-101/Ablation/Batch Size 1e-4 1/BS{}/'.format(batch_size)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        if os.path.exists(save_dir+'Data_' + str(it)):\n",
    "            # Load precomputed results\n",
    "            data_file = open(save_dir + 'Data_' + str(it),'rb')\n",
    "            input_data = pkl.load(data_file)\n",
    "            score = input_data[\"score\"]\n",
    "            accs_mean = input_data[\"accs_mean\"]\n",
    "            nparams = input_data[\"nparams\"]\n",
    "        else:\n",
    "            accs_mean = []\n",
    "            accs_min = []\n",
    "            accs_max = []\n",
    "            nparams = []\n",
    "            score = []\n",
    "            for i in trange(5000):\n",
    "                uid = searchspace[i]\n",
    "                network = searchspace.get_network(uid)\n",
    "                network = network.to(device)\n",
    "                preds = []\n",
    "                for weight in [weight_l, weight_h]:\n",
    "\n",
    "                    torch.cuda.empty_cache()\n",
    "                    prepare_seed(21)\n",
    "\n",
    "                    # Initialize\n",
    "                    def initialize_resnet(m):\n",
    "                        if type(m) == torch.nn.Sequential:\n",
    "                            for sub_m in m:\n",
    "                                initialize_resnet(sub_m)\n",
    "                        else:\n",
    "                            fill_bias = False\n",
    "                            if hasattr(m, 'bias'):\n",
    "                                if m.bias is not None:\n",
    "                                    fill_bias = True\n",
    "\n",
    "                            if fill_bias:\n",
    "                                torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "                            fill_weight = False\n",
    "                            if hasattr(m, 'weight'):\n",
    "                                fill_weight = True\n",
    "\n",
    "                            if hasattr(m, 'affine'):\n",
    "                                if not m.affine:\n",
    "                                    fill_weight = False\n",
    "\n",
    "                            if fill_weight:\n",
    "                                torch.nn.init.constant_(m.weight, weight)\n",
    "\n",
    "                    network.apply(initialize_resnet)\n",
    "                    y_pred, _ = network(x)\n",
    "                    pred = y_pred.cpu().detach().numpy().flatten()\n",
    "                    pred_min = np.nanmin(pred)\n",
    "                    pred_max = np.nanmax(pred)\n",
    "                    pred_norm = (pred - pred_min)/(pred_max - pred_min)\n",
    "                    preds.append(pred_norm)\n",
    "\n",
    "                # Compute the score\n",
    "                preds = np.array(preds)\n",
    "                preds[np.where(preds==0)] = np.nan\n",
    "                mae = np.nanmean(np.abs(preds[0,:]-preds[1,:]))\n",
    "                mean = np.nanmean(preds)\n",
    "\n",
    "                score.append(mae/mean)\n",
    "                nparams.append(sum(p.numel() for p in network.parameters()))\n",
    "                accs_mean.append(searchspace.get_final_accuracy(uid, acc_type, args.trainval)[0])\n",
    "\n",
    "            save_dic = {}\n",
    "            save_dic[\"score\"] = score\n",
    "            save_dic[\"accs_mean\"] = accs_mean\n",
    "            save_dic[\"nparams\"] = nparams\n",
    "            pkl.dump(save_dic, open(save_dir + \"Data_\" + str(it), \"wb\"))\n",
    "\n",
    "        stats, remain = compute_stats(score, accs_mean, raw=True)\n",
    "        batch_all_spearman.append(stats[0])\n",
    "        batch_top10_spearman.append(stats[1])\n",
    "        batch_all_kendall.append(stats[2])\n",
    "        batch_top10_kendall.append(stats[3])\n",
    "        batch_top10top10.append(stats[4])\n",
    "        batch_top64top5.append(stats[5])\n",
    "\n",
    "    batch_all_spearman_all.append(batch_all_spearman)\n",
    "    batch_top10_spearman_all.append(batch_top10_spearman)\n",
    "    batch_all_kendall_all.append(batch_all_kendall)\n",
    "    batch_top10_kendall_all.append(batch_top10_kendall)\n",
    "    batch_top10top10_all.append(batch_top10top10)\n",
    "    batch_top64top5_all.append(batch_top64top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "def plot_experiment(exp_list, name, filename):\n",
    "    fig = plt.figure(figsize=(7.2,4.45))\n",
    "    ax = fig.add_subplot(111)\n",
    "    def plot_exp(exp, label):\n",
    "        exp = np.array(exp)\n",
    "        q_75 = np.nanquantile(exp, .75, axis=1)\n",
    "        q_25 = np.nanquantile(exp, .25, axis=1)\n",
    "        mean = np.nanmedian(exp, axis=1)\n",
    "        ax.plot(range(len(mean)), mean, label=label)\n",
    "        ax.fill_between(range(len(mean)), np.nanmin(exp, axis=1), np.nanmax(exp, axis=1), alpha=0.1)\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "#         ax.set_xticklabels(['','8','16','32'])\n",
    "        ax.set_xticklabels(['','8','16','32','64','128','256','512','1024'])\n",
    "        for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "                     ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            item.set_fontsize(16)\n",
    "    for exp,ename in exp_list:\n",
    "        plot_exp(exp,ename)\n",
    "    plt.grid()\n",
    "    plt.xlabel('Batch size', fontsize=22)\n",
    "    if name=='rho':\n",
    "        plt.ylabel(r'Spearman $\\rho$', fontsize=22)\n",
    "    elif name=='tau':\n",
    "        plt.ylabel(r'Kendall $\\tau$', fontsize=18)\n",
    "#     plt.ylim(0.3,1)\n",
    "    plt.legend(loc=4, fontsize=20)\n",
    "    plt.savefig(filename,\n",
    "                bbox_inches='tight', \n",
    "                dpi=300,\n",
    "                format='pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = './release_results/NAS-Bench-201/IMAGENET16-120/Ablation/Batch Size/'\n",
    "filename = save_dir+\"BatchSize_Spearman_201_IMAGENET.eps\"\n",
    "plot_experiment([(batch_all_spearman_all,'Overall'), (batch_top10_spearman_all,'Top-10%')], 'rho', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = './release_results/NAS-Bench-201/IMAGENET16-120/Ablation/Batch Size/'\n",
    "filename = save_dir+\"BatchSize_Kendall_201_IMAGENET.eps\"\n",
    "plot_experiment([(batch_all_kendall_all,'Overall'), (batch_top10_kendall_all,'Top-10%')], 'tau', filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
