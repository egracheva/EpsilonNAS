{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduction of  `epsilon` metric\n",
    "\n",
    "If you wish to reproduce the results presented in our paper from scratch, feel free to use the below code.\n",
    "In this notebook, we provide the codes to reproduce the results for NAS-Bench-101 sarch space, CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 09:17:08.448522: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.1/extras/CUPTI/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.1/extras/CUPTI/lib64\n",
      "2023-03-15 09:17:08.448643: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.1/extras/CUPTI/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.1/extras/CUPTI/lib64\n",
      "2023-03-15 09:17:08.448655: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from tqdm import trange\n",
    "from dotmap import DotMap\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import nasspace\n",
    "from datasets import data\n",
    "from epsilon_utils import prepare_seed, compute_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'cifar10'\n",
    "data_loc = './datasets/cifardata'\n",
    "batch_size = 256\n",
    "repeat = 1\n",
    "GPU = '0'\n",
    "augtype = 'none'\n",
    "trainval = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments required for NAS-Bench-201 search space initialisation\n",
    "args = DotMap()\n",
    "\n",
    "args.api_loc = './nasbench_only108.tfrecord'\n",
    "args.nasspace = 'nasbench101'\n",
    "args.dataset = dataset\n",
    "args.stem_out_channels = 128\n",
    "args.num_stacks = 3\n",
    "args.num_modules_per_stack = 3\n",
    "args.num_labels = 1\n",
    "\n",
    "savedataset = dataset\n",
    "dataset = 'fake' if 'fake' in savedataset else savedataset\n",
    "savedataset = savedataset.replace('fake', '')\n",
    "if savedataset == 'cifar10':\n",
    "    savedataset = savedataset + '-valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from file... This may take a few minutes...\n",
      "WARNING:tensorflow:From /home/gracheva/Work/NAS/Epsilon-NAS/NAS-Bench-101/nasbench/api.py:146: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "Loaded dataset in 73 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load the search space (it takes some time)\n",
    "searchspace = nasspace.get_search_space(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'valid' in savedataset:\n",
    "    savedataset = savedataset.replace('-valid', '')\n",
    "    \n",
    "if args.dataset == 'cifar10':\n",
    "    acc_type = 'ori-test'\n",
    "    val_acc_type = 'x-valid'\n",
    "else:\n",
    "    acc_type = 'x-test'\n",
    "    val_acc_type = 'x-valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Randomly select n_samples architectures\n",
    "prepare_seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load the data \n",
    "train_loader = data.get_data(dataset, data_loc, trainval, batch_size, augtype, repeat, args)\n",
    "\n",
    "# Pick up a batch\n",
    "data_iterator = iter(train_loader)\n",
    "x, _= next(data_iterator)\n",
    "x = x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 423624/423624 [33:58:14<00:00,  3.46it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dir = './release_results_ICML_11/{}/'.format(dataset.upper(), batch_size)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "weights = [-1, 1]\n",
    "test_accs_mean = []\n",
    "test_accs_min = []\n",
    "test_accs_max = []\n",
    "val_accs_mean = []\n",
    "val_accs_min = []\n",
    "val_accs_max = []\n",
    "nparams = []\n",
    "scores = []\n",
    "times = []\n",
    "\n",
    "for i in trange(len(searchspace)):\n",
    "    start = time.time()\n",
    "    uid = searchspace[i]\n",
    "    network = searchspace.get_network(uid)\n",
    "    network = network.to(device)\n",
    "    score = compute_epsilon(x, network, weights)\n",
    "    scores.append(score)\n",
    "    nparams.append(sum(p.numel() for p in network.parameters()))\n",
    "    test_accs_mean.append(searchspace.get_final_accuracy(uid, acc_type, False)[0])\n",
    "    test_accs_min.append(searchspace.get_final_accuracy(uid, acc_type, False)[1])\n",
    "    test_accs_max.append(searchspace.get_final_accuracy(uid, acc_type, False)[2])\n",
    "    if dataset=='cifar10':\n",
    "        val_accs_mean.append(searchspace.get_final_accuracy(uid, val_acc_type, True)[0])\n",
    "        val_accs_min.append(searchspace.get_final_accuracy(uid, val_acc_type, True)[1])\n",
    "        val_accs_max.append(searchspace.get_final_accuracy(uid, val_acc_type, True)[2])\n",
    "    else:\n",
    "        val_accs_mean.append(searchspace.get_final_accuracy(uid, val_acc_type, False)[0])\n",
    "        val_accs_min.append(searchspace.get_final_accuracy(uid, val_acc_type, False)[1])\n",
    "        val_accs_max.append(searchspace.get_final_accuracy(uid, val_acc_type, False)[2])\n",
    "    times.append(time.time()-start)\n",
    "\n",
    "# Save your results\n",
    "save_dic = {} \n",
    "save_dic[\"scores\"] = scores\n",
    "save_dic[\"test_accs_mean\"] = test_accs_mean\n",
    "save_dic[\"test_accs_min\"] =  test_accs_min\n",
    "save_dic[\"test_accs_max\"] =  test_accs_max\n",
    "save_dic[\"val_accs_mean\"] = val_accs_mean\n",
    "save_dic[\"val_accs_min\"] =  val_accs_min\n",
    "save_dic[\"val_accs_max\"] =  val_accs_max\n",
    "save_dic[\"nparams\"] = nparams\n",
    "save_dic[\"times\"] = times\n",
    "\n",
    "pkl.dump(save_dic, open(save_dir + \"Data\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./release_results/NAS-Bench-101/CIFAR10/INDIVIDUAL_WEIGHTS_1e-7_1/BS_256/NoInf/BatchMean/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your results\n",
    "save_dir = './release_results/NAS-Bench-101/CIFAR10/WEIGHT_{}_{}/BS_{}/'.format(weight_l, weight_h, args.batch_size)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "save_dic = {}\n",
    "save_dic[\"score\"] = score\n",
    "save_dic[\"accs_mean\"] = accs_mean\n",
    "save_dic[\"accs_min\"] = accs_min\n",
    "save_dic[\"accs_max\"] = accs_max\n",
    "save_dic[\"nparams\"] = nparams\n",
    "    \n",
    "pkl.dump(save_dic, open(save_dir + \"Data_1\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
